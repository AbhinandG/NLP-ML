# -*- coding: utf-8 -*-
"""MovieReviewsIMDB.ipynb

Automatically generated by Colaboratory.

"""





"""**IMPORTING ALL LIBRARIES**


"""

from textblob import TextBlob
import sys
import tweepy
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import os
import nltk
nltk.download('vader_lexicon')
from sklearn.metrics import confusion_matrix, classification_report
nltk.download('punkt')
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import BernoulliNB
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
import re
import string
from wordcloud import WordCloud, STOPWORDS
from PIL import Image
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.stem import SnowballStemmer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from sklearn.feature_extraction.text import CountVectorizer

import requests

from bs4 import BeautifulSoup

import seaborn as sns

import csv

"""**WEB-SCRAPING THE DETAILS FOR "Paraiyerunm Perumal" FROM IMDB**"""

URL="https://www.imdb.com/title/tt8176054/reviews/?ref_=tt_ql_urv"
r=requests.get(URL)
soup=BeautifulSoup(r.content,'html5lib')

PariyerumPerumal =[]

table = soup.find('div', attrs = {'class':'lister-list'})

for row in table.findAll('div',
                         attrs = {'class':'lister-item mode-detail imdb-user-review collapsable'}):
    quote = {}
    for anotherrow in row.findAll('span',attrs={'class':'display-name-link'}):
        quote['user']=anotherrow.find('a').contents

    for anotherrow in row.findAll('span',attrs={'class':'review-date'}):
        quote['date']=anotherrow.text

    for anotherrow in row.findAll('div',attrs={'class':'content'}):
        quote['review'] = anotherrow.text
        PariyerumPerumal.append(quote)

PariyerumPerumalDF=pd.DataFrame(PariyerumPerumal)

"""**DATA CLEANING BY REMOVING STOPWORDS, NEWLINE CHARACTERS, REMVOVING BRACKETS, NUMBERS, ETC**"""

stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',
             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',
             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do','The',
             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',
             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here','help ful',
             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in','review','movie','film','Movie',
             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',' helpful','helpful\'',
             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once','found','but','not','story','watch','one','film','lower','make','much',
             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', "shes", 'should', "shouldve",'so', 'some', 'such','helpful ',
             't', 'than', 'that', "thatll", 'the', 'their', 'theirs', 'them',
             'themselves', 'then', 'there', 'these', 'they', 'this', 'those','good','class','excellent','movies','scenes','cinema','vote','Sign','Permalink','helpful','Please','no','plot','Kanchana',
             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was','was','Was','well','Well','many',
             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom','How','Just','It','it','I','We','i','pariyerum','perumal','Pariyerum','Perumal','Asal','asal','Jai','jai','Bhim','bhim','Asuran','asuran','him','help','Kachana','kanchana','Kaithi','kaithi',
             'why', 'will', 'with', 'won', 'y', 'you', "youd","youll", "youre",
             "youve", 'your', 'yours','must', 'yourself', 'yourselves','This']


STOPWORDS = set(stopwordlist)

def cleaning_brackets(text):
    return re.sub('[^a-zA-Z ]','',str(text))
PariyerumPerumalDF.iloc[:,0] = PariyerumPerumalDF.iloc[:,0].apply(lambda text: cleaning_brackets(text))

def cleaning_newline(text):
    return re.sub('\n','',str(text))
PariyerumPerumalDF.iloc[:,2]=PariyerumPerumalDF.iloc[:,2].apply(lambda text: cleaning_newline(text))

def cleaning_stopwords(text):
    return " ".join([word for word in str(text).split() if word not in STOPWORDS])
PariyerumPerumalDF.iloc[:,2] = PariyerumPerumalDF.iloc[:,2].apply(lambda text: cleaning_stopwords(text))

def cleaning_numbers(text):
    return re.sub('[0-9|\(\)|:]',' ',str(text))
PariyerumPerumalDF.iloc[:,2]=PariyerumPerumalDF.iloc[:,2].apply(lambda text: cleaning_numbers(text))

PariyerumPerumalDF['movie']='Pariyerum Perumal'

"""**WEB-SCRAPING THE DETAILS FOR "Asal" FROM IMDB**"""

URL="https://www.imdb.com/title/tt1483821/reviews/?ref_=tt_ql_urv"
r=requests.get(URL)
soup=BeautifulSoup(r.content,'html5lib')

Asal =[]

table = soup.find('div', attrs = {'class':'lister-list'})

for row in table.findAll('div',
                         attrs = {'class':'lister-item mode-detail imdb-user-review collapsable'}):
    quote = {}
    for anotherrow in row.findAll('span',attrs={'class':'display-name-link'}):
        quote['user']=anotherrow.find('a').contents

    for anotherrow in row.findAll('span',attrs={'class':'review-date'}):
        quote['date']=anotherrow.text

    for anotherrow in row.findAll('div',attrs={'class':'content'}):
        quote['review'] = anotherrow.text
        Asal.append(quote)

AsalDF=pd.DataFrame(Asal)

def cleaning_brackets(text):
    return re.sub('[^a-zA-Z ]','',str(text))
AsalDF.iloc[:,0] = AsalDF.iloc[:,0].apply(lambda text: cleaning_brackets(text))

def cleaning_newline(text):
    return re.sub('\n','',str(text))
AsalDF.iloc[:,2]=AsalDF.iloc[:,2].apply(lambda text: cleaning_newline(text))

def cleaning_stopwords(text):
    return " ".join([word for word in str(text).split() if word not in STOPWORDS])
AsalDF.iloc[:,2] = AsalDF.iloc[:,2].apply(lambda text: cleaning_stopwords(text))

def cleaning_numbers(text):
    return re.sub('[0-9|\(\)|:]',' ',str(text))
AsalDF.iloc[:,2]=AsalDF.iloc[:,2].apply(lambda text: cleaning_numbers(text))

AsalDF['movie']='Asal'

"""**WEB-SCRAPING THE DETAILS FOR "Jai Bhim" FROM IMDB**"""

URL="https://www.imdb.com/title/tt15097216/reviews/?ref_=tt_ql_urv"
r=requests.get(URL)
soup=BeautifulSoup(r.content,'html5lib')

JaiBhim =[]

table = soup.find('div', attrs = {'class':'lister-list'})

for row in table.findAll('div',
                         attrs = {'class':'lister-item mode-detail imdb-user-review collapsable'}):
    quote = {}
    for anotherrow in row.findAll('span',attrs={'class':'display-name-link'}):
        quote['user']=anotherrow.find('a').contents

    for anotherrow in row.findAll('span',attrs={'class':'review-date'}):
        quote['date']=anotherrow.text

    for anotherrow in row.findAll('div',attrs={'class':'content'}):
        quote['review'] = anotherrow.text
        JaiBhim.append(quote)

JaiBhimDF=pd.DataFrame(JaiBhim)

def cleaning_brackets(text):
    return re.sub('[^a-zA-Z ]','',str(text))
JaiBhimDF.iloc[:,0] = JaiBhimDF.iloc[:,0].apply(lambda text: cleaning_brackets(text))

def cleaning_newline(text):
    return re.sub('\n','',str(text))
JaiBhimDF.iloc[:,2]=JaiBhimDF.iloc[:,2].apply(lambda text: cleaning_newline(text))

def cleaning_stopwords(text):
    return " ".join([word for word in str(text).split() if word not in STOPWORDS])
JaiBhimDF.iloc[:,2] = JaiBhimDF.iloc[:,2].apply(lambda text: cleaning_stopwords(text))

def cleaning_numbers(text):
    return re.sub('[0-9|\(\)|:]',' ',str(text))
JaiBhimDF.iloc[:,2]=JaiBhimDF.iloc[:,2].apply(lambda text: cleaning_numbers(text))

JaiBhimDF['movie']='Jai Bhim'

"""**WEB-SCRAPING THE DETAILS FOR "Asuran" FROM IMDB**"""

URL="https://www.imdb.com/title/tt9477520/reviews/?ref_=tt_ql_urv"
r=requests.get(URL)
soup=BeautifulSoup(r.content,'html5lib')

Asuran =[]

table = soup.find('div', attrs = {'class':'lister-list'})

for row in table.findAll('div',
                         attrs = {'class':'lister-item mode-detail imdb-user-review collapsable'}):
    quote = {}
    for anotherrow in row.findAll('span',attrs={'class':'display-name-link'}):
        quote['user']=anotherrow.find('a').contents

    for anotherrow in row.findAll('span',attrs={'class':'review-date'}):
        quote['date']=anotherrow.text

    for anotherrow in row.findAll('div',attrs={'class':'content'}):
        quote['review'] = anotherrow.text
        Asuran.append(quote)

AsuranDF=pd.DataFrame(Asuran)

def cleaning_brackets(text):
    return re.sub('[^a-zA-Z ]','',str(text))
AsuranDF.iloc[:,0] = AsuranDF.iloc[:,0].apply(lambda text: cleaning_brackets(text))

def cleaning_newline(text):
    return re.sub('\n','',str(text))
AsuranDF.iloc[:,2]=AsuranDF.iloc[:,2].apply(lambda text: cleaning_newline(text))

def cleaning_stopwords(text):
    return " ".join([word for word in str(text).split() if word not in STOPWORDS])
AsuranDF.iloc[:,2] = AsuranDF.iloc[:,2].apply(lambda text: cleaning_stopwords(text))

def cleaning_numbers(text):
    return re.sub('[0-9|\(\)|:]',' ',str(text))
AsuranDF.iloc[:,2]=AsuranDF.iloc[:,2].apply(lambda text: cleaning_numbers(text))

AsuranDF['movie']='Asuran'

"""
**WEB-SCRAPING THE DETAILS FOR "Kaithi" FROM IMDB**"""

URL="https://www.imdb.com/title/tt9900782/reviews/?ref_=tt_ql_urv"
r=requests.get(URL)
soup=BeautifulSoup(r.content,'html5lib')

Kaithi =[]

table = soup.find('div', attrs = {'class':'lister-list'})

for row in table.findAll('div',
                         attrs = {'class':'lister-item mode-detail imdb-user-review collapsable'}):
    quote = {}
    for anotherrow in row.findAll('span',attrs={'class':'display-name-link'}):
        quote['user']=anotherrow.find('a').contents

    for anotherrow in row.findAll('span',attrs={'class':'review-date'}):
        quote['date']=anotherrow.text

    for anotherrow in row.findAll('div',attrs={'class':'content'}):
        quote['review'] = anotherrow.text
        Kaithi.append(quote)

KaithiDF=pd.DataFrame(Kaithi)

def cleaning_brackets(text):
    return re.sub('[^a-zA-Z ]','',str(text))
KaithiDF.iloc[:,0] = KaithiDF.iloc[:,0].apply(lambda text: cleaning_brackets(text))

def cleaning_newline(text):
    return re.sub('\n','',str(text))
KaithiDF.iloc[:,2]=KaithiDF.iloc[:,2].apply(lambda text: cleaning_newline(text))

def cleaning_stopwords(text):
    return " ".join([word for word in str(text).split() if word not in STOPWORDS])
KaithiDF.iloc[:,2] = KaithiDF.iloc[:,2].apply(lambda text: cleaning_stopwords(text))

def cleaning_numbers(text):
    return re.sub('[0-9|\(\)|:]',' ',str(text))
KaithiDF.iloc[:,2]=KaithiDF.iloc[:,2].apply(lambda text: cleaning_numbers(text))

KaithiDF['movie']='Kaithi'

"""**WEB-SCRAPING THE DETAILS FOR "Kanchana 3" FROM IMDB**"""

URL="https://www.imdb.com/title/tt8042248/reviews/?ref_=tt_ql_urv"
r=requests.get(URL)
soup=BeautifulSoup(r.content,'html5lib')

Kanchana3 =[]

table = soup.find('div', attrs = {'class':'lister-list'})

for row in table.findAll('div',
                         attrs = {'class':'lister-item mode-detail imdb-user-review collapsable'}):
    quote = {}
    for anotherrow in row.findAll('span',attrs={'class':'display-name-link'}):
        quote['user']=anotherrow.find('a').contents

    for anotherrow in row.findAll('span',attrs={'class':'review-date'}):
        quote['date']=anotherrow.text

    for anotherrow in row.findAll('div',attrs={'class':'content'}):
        quote['review'] = anotherrow.text
        Kanchana3.append(quote)

Kanchana3DF=pd.DataFrame(Kanchana3)

def cleaning_brackets(text):
    return re.sub('[^a-zA-Z ]','',str(text))
Kanchana3DF.iloc[:,0] = Kanchana3DF.iloc[:,0].apply(lambda text: cleaning_brackets(text))

def cleaning_newline(text):
    return re.sub('\n','',str(text))
Kanchana3DF.iloc[:,2]=Kanchana3DF.iloc[:,2].apply(lambda text: cleaning_newline(text))

def cleaning_stopwords(text):
    return " ".join([word for word in str(text).split() if word not in STOPWORDS])
Kanchana3DF.iloc[:,2] = Kanchana3DF.iloc[:,2].apply(lambda text: cleaning_stopwords(text))

def cleaning_numbers(text):
    return re.sub('[0-9|\(\)|:]',' ',str(text))
Kanchana3DF.iloc[:,2]=Kanchana3DF.iloc[:,2].apply(lambda text: cleaning_numbers(text))

Kanchana3DF['movie']='Kanchana 3'

"""
**WEB-SCRAPING THE DETAILS FOR "Pisasu" FROM IMDB**"""

URL="https://www.imdb.com/title/tt4290746/reviews/?ref_=tt_ql_urv"
r=requests.get(URL)
soup=BeautifulSoup(r.content,'html5lib')

Pisasu =[]

table = soup.find('div', attrs = {'class':'lister-list'})

for row in table.findAll('div',
                         attrs = {'class':'lister-item mode-detail imdb-user-review collapsable'}):
    quote = {}
    for anotherrow in row.findAll('span',attrs={'class':'display-name-link'}):
        quote['user']=anotherrow.find('a').contents

    for anotherrow in row.findAll('span',attrs={'class':'review-date'}):
        quote['date']=anotherrow.text

    for anotherrow in row.findAll('div',attrs={'class':'content'}):
        quote['review'] = anotherrow.text
        Pisasu.append(quote)

PisasuDF=pd.DataFrame(Pisasu)

def cleaning_brackets(text):
    return re.sub('[^a-zA-Z ]','',str(text))
PisasuDF.iloc[:,0] = PisasuDF.iloc[:,0].apply(lambda text: cleaning_brackets(text))

def cleaning_newline(text):
    return re.sub('\n','',str(text))
PisasuDF.iloc[:,2]=PisasuDF.iloc[:,2].apply(lambda text: cleaning_newline(text))

def cleaning_stopwords(text):
    return " ".join([word for word in str(text).split() if word not in STOPWORDS])
PisasuDF.iloc[:,2] = PisasuDF.iloc[:,2].apply(lambda text: cleaning_stopwords(text))

def cleaning_numbers(text):
    return re.sub('[0-9|\(\)|:]',' ',str(text))
PisasuDF.iloc[:,2]=PisasuDF.iloc[:,2].apply(lambda text: cleaning_numbers(text))

PisasuDF['movie']='Pisasu'

"""
**WEB-SCRAPING THE DETAILS FOR "Zombie" FROM IMDB**"""

URL="https://www.imdb.com/title/tt10958336/reviews/?ref_=tt_ql_urv"
r=requests.get(URL)
soup=BeautifulSoup(r.content,'html5lib')

Zombie =[]

table = soup.find('div', attrs = {'class':'lister-list'})

for row in table.findAll('div',
                         attrs = {'class':'lister-item mode-detail imdb-user-review collapsable'}):
    quote = {}
    for anotherrow in row.findAll('span',attrs={'class':'display-name-link'}):
        quote['user']=anotherrow.find('a').contents

    for anotherrow in row.findAll('span',attrs={'class':'review-date'}):
        quote['date']=anotherrow.text

    for anotherrow in row.findAll('div',attrs={'class':'content'}):
        quote['review'] = anotherrow.text
        Zombie.append(quote)

ZombieDF=pd.DataFrame(Zombie)

def cleaning_brackets(text):
    return re.sub('[^a-zA-Z ]','',str(text))
ZombieDF.iloc[:,0] = ZombieDF.iloc[:,0].apply(lambda text: cleaning_brackets(text))

def cleaning_newline(text):
    return re.sub('\n','',str(text))
ZombieDF.iloc[:,2]=ZombieDF.iloc[:,2].apply(lambda text: cleaning_newline(text))

def cleaning_stopwords(text):
    return " ".join([word for word in str(text).split() if word not in STOPWORDS])
ZombieDF.iloc[:,2] = ZombieDF.iloc[:,2].apply(lambda text: cleaning_stopwords(text))

def cleaning_numbers(text):
    return re.sub('[0-9|\(\)|:]',' ',str(text))
ZombieDF.iloc[:,2]=ZombieDF.iloc[:,2].apply(lambda text: cleaning_numbers(text))

ZombieDF['movie']='Zombie'

"""
**WEB-SCRAPING THE DETAILS FOR "Darbar" FROM IMDB**"""

URL="https://www.imdb.com/title/tt9635540/reviews?spoiler=hide&sort=userRating&dir=asc&ratingFilter=0"
r=requests.get(URL)
soup=BeautifulSoup(r.content,'html5lib')

Darbar =[]

table = soup.find('div', attrs = {'class':'lister-list'})

for row in table.findAll('div',
                         attrs = {'class':'lister-item mode-detail imdb-user-review collapsable'}):
    quote = {}
    for anotherrow in row.findAll('span',attrs={'class':'display-name-link'}):
        quote['user']=anotherrow.find('a').contents

    for anotherrow in row.findAll('span',attrs={'class':'review-date'}):
        quote['date']=anotherrow.text

    for anotherrow in row.findAll('div',attrs={'class':'content'}):
        quote['review'] = anotherrow.text
        Darbar.append(quote)

DarbarDF=pd.DataFrame(Darbar)

def cleaning_brackets(text):
    return re.sub('[^a-zA-Z ]','',str(text))
DarbarDF.iloc[:,0] = DarbarDF.iloc[:,0].apply(lambda text: cleaning_brackets(text))

def cleaning_newline(text):
    return re.sub('\n','',str(text))
DarbarDF.iloc[:,2]=DarbarDF.iloc[:,2].apply(lambda text: cleaning_newline(text))

def cleaning_stopwords(text):
    return " ".join([word for word in str(text).split() if word not in STOPWORDS])
DarbarDF.iloc[:,2] = DarbarDF.iloc[:,2].apply(lambda text: cleaning_stopwords(text))

def cleaning_numbers(text):
    return re.sub('[0-9|\(\)|:]',' ',str(text))
DarbarDF.iloc[:,2]=DarbarDF.iloc[:,2].apply(lambda text: cleaning_numbers(text))

DarbarDF['movie']='Darbar'

"""
**WEB-SCRAPING THE DETAILS FOR "Master" FROM IMDB**"""

URL="https://www.imdb.com/title/tt10579952/reviews?spoiler=hide&sort=userRating&dir=asc&ratingFilter=0"
r=requests.get(URL)
soup=BeautifulSoup(r.content,'html5lib')

Master =[]

table = soup.find('div', attrs = {'class':'lister-list'})

for row in table.findAll('div',
                         attrs = {'class':'lister-item mode-detail imdb-user-review collapsable'}):
    quote = {}
    for anotherrow in row.findAll('span',attrs={'class':'display-name-link'}):
        quote['user']=anotherrow.find('a').contents

    for anotherrow in row.findAll('span',attrs={'class':'review-date'}):
        quote['date']=anotherrow.text

    for anotherrow in row.findAll('div',attrs={'class':'content'}):
        quote['review'] = anotherrow.text
        Master.append(quote)

MasterDF=pd.DataFrame(Master)

def cleaning_brackets(text):
    return re.sub('[^a-zA-Z ]','',str(text))
MasterDF.iloc[:,0] = MasterDF.iloc[:,0].apply(lambda text: cleaning_brackets(text))

def cleaning_newline(text):
    return re.sub('\n','',str(text))
MasterDF.iloc[:,2]=MasterDF.iloc[:,2].apply(lambda text: cleaning_newline(text))

def cleaning_stopwords(text):
    return " ".join([word for word in str(text).split() if word not in STOPWORDS])
MasterDF.iloc[:,2] = MasterDF.iloc[:,2].apply(lambda text: cleaning_stopwords(text))

def cleaning_numbers(text):
    return re.sub('[0-9|\(\)|:]',' ',str(text))
MasterDF.iloc[:,2]= MasterDF.iloc[:,2].apply(lambda text: cleaning_numbers(text))

MasterDF['movie']='Master'

"""**CONCATENATING ALL THE DATAFRAMES**"""

merged_df = pd.concat([PariyerumPerumalDF, JaiBhimDF, AsuranDF, AsalDF, KaithiDF, Kanchana3DF,PisasuDF, ZombieDF, DarbarDF, MasterDF])
merged_df.reset_index(inplace=True, drop=True)

merged_df

"""**SENTIMENT ANALYSIS USING TEXTBLOB**"""

for i in range(188):
    merged_df.loc[i,'polarity']=TextBlob(merged_df.loc[i,'review']).sentiment[0]
    merged_df.loc[i,'subjectivity']=TextBlob(merged_df.loc[i,'review']).sentiment[1]

for i in range(188):
    if merged_df.loc[i,'polarity']>0.1:
        merged_df.loc[i,'sentiment']='positive'
    elif merged_df.loc[i,'polarity']<=0.1:
        merged_df.loc[i,'sentiment']='negative'
    else:
        merged_df.loc[i,'sentiment']='neutral'

"""**EXPLORATORY DATA ANALYSIS**"""

merged_df['sentiment'].value_counts()

sns.countplot(x='sentiment', data=merged_df)

modified_df1 = merged_df.groupby(["movie","sentiment"]).size().reset_index(name='counts')

modified_df1

modified_df1.pivot_table(index = 'movie', columns = 'sentiment',values='counts' ).plot(kind = 'bar', stacked = True)

from pylab import rcParams
rcParams['figure.figsize'] = 15, 5

for i in range(188):
    leng=len(merged_df.iloc[i,1])
    item=merged_df.iloc[i,1]
    merged_df.iloc[i,1]=item[leng-4:leng:1]

modified_df2 = merged_df.groupby(["date","sentiment"]).size().reset_index(name='counts')

modified_df2 = modified_df2.sort_values(by=['date'], ascending=True)

modified_df2.pivot_table(index = 'date', columns = 'sentiment',values='counts' ).plot(kind = 'line', stacked = True)

modified_df3=pd.to_numeric(merged_df['polarity']).groupby(merged_df['date'])
modified_df3=modified_df3.mean()
modified_df3=pd.DataFrame(modified_df3)
modified_df4=pd.to_numeric(merged_df['subjectivity']).groupby(merged_df['date'])
modified_df4=modified_df4.mean()
modified_df4=pd.DataFrame(modified_df4)
modified_df5=pd.merge(
    modified_df3,
    modified_df4,
    how="inner",
    on=None,
    left_on="date",
    right_on="date",
    sort=True,
    suffixes=("_x", "_y"),
    copy=True,
    indicator=False,
    validate=None,
)
sns.lineplot(data=modified_df5)

"""*TOKENISATION*"""

import string
english_punctuations = string.punctuation
punctuations_list = english_punctuations
def cleaning_punctuations(text):
    translator = str.maketrans('', '', punctuations_list)
    return text.translate(translator)
merged_df['review']= merged_df['review'].apply(lambda x: cleaning_punctuations(x))

merged_df['review'] = merged_df.apply(lambda row: nltk.word_tokenize(row['review']), axis=1)

merged_df_tokenised=pd.DataFrame()
merged_df_tokenised['review']=merged_df['review']
merged_df_tokenised['movie']=merged_df['movie']

for i in range(188):
    if merged_df.iloc[i,6]=='positive':
        merged_df.loc[i,'targetval']=1
    else:
        merged_df.loc[i,'targetval']=0

data=merged_df[['targetval','review']]
data

"""
**VECTORISATION USING TF-IDF**"""

def get_feature_vector(train_fit):
    vector = TfidfVectorizer(sublinear_tf=True)
    vector.fit(train_fit)
    return vector

tf_vector = get_feature_vector(np.array(data.iloc[:, 1].astype(str)).ravel())
X = tf_vector.transform(np.array(data.iloc[:, 1].astype(str)).ravel())
y = np.array(data.iloc[:, 0]).ravel()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30)

"""
**CLASSIFICATION MODELS**"""

def model_Evaluate(model):

    y_pred = model.predict(X_test)

    print(classification_report(y_test, y_pred))

    cf_matrix = confusion_matrix(y_test, y_pred)
    categories = ['Negative','Positive']
    group_names = ['True Neg','False Pos', 'False Neg','True Pos']
    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]
    labels = [f'{v1}n{v2}' for v1, v2 in zip(group_names,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)
    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',
    xticklabels = categories, yticklabels = categories)
    plt.xlabel("Predicted values", fontdict = {'size':14}, labelpad = 10)
    plt.ylabel("Actual values" , fontdict = {'size':14}, labelpad = 10)
    plt.title ("Confusion Matrix", fontdict = {'size':18}, pad = 20)

"""
**LOGISTIC REGRESSION**"""

#LOGISTIC REGRESSION

from sklearn.metrics import accuracy_score

LR_model = LogisticRegression(solver='lbfgs')
LR_model.fit(X_train, y_train)
model_Evaluate(LR_model)
y_predict_lr = LR_model.predict(X_test)
print("ACCURACY SCORE IS: "+str(accuracy_score(y_test, y_predict_lr)))

#AUC CURVE FOR LOGISTIC REGRESSION
from sklearn.metrics import roc_curve, auc
fpr, tpr, thresholds = roc_curve(y_test, y_predict_lr)
roc_auc = auc(fpr, tpr)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC CURVE')
plt.legend(loc="lower right")
plt.show()

"""
**BNB**"""

#BERNOULLI NAIVES BAYES

BNBmodel=BernoulliNB()
BNBmodel.fit(X_train,y_train)
model_Evaluate(BNBmodel)
y_predict_bnb=BNBmodel.predict(X_test)
print("ACCURACY SCORE IS: "+str(accuracy_score(y_test,y_predict_bnb)))

#AUC CURVE FOR BERNOULLI
from sklearn.metrics import roc_curve, auc
fpr, tpr, thresholds = roc_curve(y_test, y_predict_bnb)
roc_auc = auc(fpr, tpr)
plt.figure()
plt.plot(fpr, tpr, color='red', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC CURVE')
plt.legend(loc="lower right")
plt.show()

"""
**SVM**"""

#SUPPORT VECTOR MACHINE

SVCmodel = LinearSVC()
SVCmodel.fit(X_train, y_train)
model_Evaluate(SVCmodel)
y_predict_svm = SVCmodel.predict(X_test)
print(accuracy_score(y_test,y_predict_svm))

#ROC-AUC FOR SVM
from sklearn.metrics import roc_curve, auc
fpr, tpr, thresholds = roc_curve(y_test, y_predict_svm)
roc_auc = auc(fpr, tpr)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC CURVE')
plt.legend(loc="lower right")
plt.show()

"""**WORD CLOUDS**"""

#Pariyerum perumal
merged_df_tokenised1=merged_df_tokenised['review'][0:24]
wc = WordCloud(max_words = 1000 , width = 1600 , height = 800, stopwords=stopwordlist,
               collocations=False).generate(",".join(str(v) for v in merged_df_tokenised1))
plt.imshow(wc)

#Jai Bhim
merged_df_tokenised2=merged_df_tokenised['review'][25:44]
wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,stopwords=stopwordlist,
               collocations=False).generate(','.join(str(v) for v in merged_df_tokenised2))
plt.imshow(wc)

#Asuran
merged_df_tokenised3=merged_df_tokenised['review'][45:67]
wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,stopwords=stopwordlist,
               collocations=False).generate(','.join(str(v) for v in merged_df_tokenised3))
plt.imshow(wc)

#Asal
merged_df_tokenised4=merged_df_tokenised['review'][68:76]
wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,stopwords=stopwordlist,
               collocations=False).generate(','.join(str(v) for v in merged_df_tokenised4))
plt.imshow(wc)

#Kaithi
merged_df_tokenised5=merged_df_tokenised['review'][77:99]
wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,stopwords=stopwordlist,
               collocations=False).generate(','.join(str(v) for v in merged_df_tokenised5))
plt.imshow(wc)

#kanchana 3
merged_df_tokenised6=merged_df_tokenised['review'][100:122]
wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,stopwords=stopwordlist,
               collocations=False).generate(','.join(str(v) for v in merged_df_tokenised6))
plt.imshow(wc)
